#!/usr/bin/env python3
"""
Inference —Å–∫—Ä–∏–ø—Ç –¥–ª—è Raspberry Pi —Å Google Coral Edge TPU.
Real-time –¥–µ—Ç–µ–∫—Ü–∏—è –¥–æ—Ä–æ–∂–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ —Å –∫–∞–º–µ—Ä—ã.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- Raspberry Pi 5 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- Google Coral USB Accelerator
- Pi Camera –∏–ª–∏ USB webcam
- TFLite –º–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è Edge TPU

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python deployment/inference.py --model models/traffic_signs_edgetpu.tflite --config configs/deployment_config.yaml
"""

import argparse
import time
import yaml
from pathlib import Path
from typing import List, Tuple
import numpy as np
import cv2

# PyCoral –¥–ª—è Edge TPU
try:
    from pycoral.adapters import common
    from pycoral.utils.edgetpu import make_interpreter
    CORAL_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  PyCoral –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU —Ä–µ–∂–∏–º.")
    CORAL_AVAILABLE = False
    import tflite_runtime.interpreter as tflite


def parse_args():
    parser = argparse.ArgumentParser(description='Inference –Ω–∞ Raspberry Pi —Å Edge TPU')
    parser.add_argument('--model', type=str, required=True,
                        help='–ü—É—Ç—å –∫ TFLite –º–æ–¥–µ–ª–∏ (Edge TPU –∏–ª–∏ –æ–±—ã—á–Ω–∞—è)')
    parser.add_argument('--config', type=str, default='configs/deployment_config.yaml',
                        help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ deployment')
    parser.add_argument('--camera', type=int, default=0,
                        help='ID –∫–∞–º–µ—Ä—ã (0 –¥–ª—è Pi Camera, 1+ –¥–ª—è USB)')
    parser.add_argument('--source', type=str, default=None,
                        help='–í–∏–¥–µ–æ —Ñ–∞–π–ª –≤–º–µ—Å—Ç–æ –∫–∞–º–µ—Ä—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    parser.add_argument('--save-video', action='store_true',
                        help='–°–æ—Ö—Ä–∞–Ω—è—Ç—å –≤–∏–¥–µ–æ —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏')
    parser.add_argument('--display', action='store_true', default=True,
                        help='–û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –≤–∏–¥–µ–æ (—Ç—Ä–µ–±—É–µ—Ç –¥–∏—Å–ø–ª–µ–π)')
    parser.add_argument('--fps-target', type=int, default=20,
                        help='–¶–µ–ª–µ–≤–æ–π FPS')
    return parser.parse_args()


def load_config(config_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML."""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_labels(config: dict) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∏–∑ dataset config."""
    dataset_config_path = 'configs/dataset.yaml'
    
    if Path(dataset_config_path).exists():
        with open(dataset_config_path, 'r') as f:
            dataset_config = yaml.safe_load(f)
            return [dataset_config['names'][i] for i in range(dataset_config['nc'])]
    else:
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        return [f"class_{i}" for i in range(20)]


class TrafficSignDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –¥–æ—Ä–æ–∂–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –Ω–∞ Edge TPU."""
    
    def __init__(self, model_path: str, use_edgetpu: bool = True):
        self.model_path = model_path
        self.use_edgetpu = use_edgetpu and CORAL_AVAILABLE
        
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞...")
        print(f"   –ú–æ–¥–µ–ª—å: {model_path}")
        print(f"   Edge TPU: {'‚úì' if self.use_edgetpu else '‚úó (CPU —Ä–µ–∂–∏–º)'}")
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
        if self.use_edgetpu:
            self.interpreter = make_interpreter(model_path)
        else:
            self.interpreter = tflite.Interpreter(model_path=model_path)
        
        self.interpreter.allocate_tensors()
        
        # –ü–æ–ª—É—á–∏—Ç—å input/output –¥–µ—Ç–∞–ª–∏
        self.input_details = self.interpreter.get_input_details()[0]
        self.output_details = self.interpreter.get_output_details()
        
        # –†–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–∞
        self.input_shape = self.input_details['shape']
        self.input_height = self.input_shape[1]
        self.input_width = self.input_shape[2]
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
        self.is_quantized = self.input_details['dtype'] == np.uint8
        
        print(f"   –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: {self.input_width}x{self.input_height}")
        print(f"   –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: {'INT8' if self.is_quantized else 'FP32'}")
        print(f"‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä –≥–æ—Ç–æ–≤!")
    
    def preprocess(self, image: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        # Resize —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º aspect ratio (letterbox)
        img_resized = cv2.resize(image, (self.input_width, self.input_height))
        
        if self.is_quantized:
            # –î–ª—è INT8 –º–æ–¥–µ–ª–∏
            img_processed = img_resized.astype(np.uint8)
        else:
            # –î–ª—è FP32 –º–æ–¥–µ–ª–∏
            img_processed = img_resized.astype(np.float32) / 255.0
        
        # –î–æ–±–∞–≤–∏—Ç—å batch dimension
        img_processed = np.expand_dims(img_processed, axis=0)
        
        return img_processed
    
    def detect(self, image: np.ndarray, confidence_threshold: float = 0.6) -> List[dict]:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ—Ç–µ–∫—Ü–∏—é."""
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        input_data = self.preprocess(image)
        
        # Inference
        self.interpreter.set_tensor(self.input_details['index'], input_data)
        self.interpreter.invoke()
        
        # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # –§–æ—Ä–º–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏ - –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –≤–∞—à YOLO output
        detections = []
        
        # –î–ª—è YOLO –æ–±—ã—á–Ω–æ –µ—Å—Ç—å boxes, scores, classes
        # –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –∏–Ω–¥–µ–∫—Å—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
        try:
            boxes = self.interpreter.get_tensor(self.output_details[0]['index'])[0]
            scores = self.interpreter.get_tensor(self.output_details[1]['index'])[0]
            classes = self.interpreter.get_tensor(self.output_details[2]['index'])[0]
            
            # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ confidence
            for i in range(len(scores)):
                if scores[i] >= confidence_threshold:
                    detections.append({
                        'bbox': boxes[i].tolist(),
                        'score': float(scores[i]),
                        'class_id': int(classes[i])
                    })
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ output: {e}")
        
        return detections


class FPSCounter:
    """–°—á–µ—Ç—á–∏–∫ FPS."""
    
    def __init__(self, window_size: int = 30):
        self.window_size = window_size
        self.timestamps = []
    
    def update(self):
        """–û–±–Ω–æ–≤–∏—Ç—å timestamp."""
        current_time = time.time()
        self.timestamps.append(current_time)
        
        # –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N timestamp'–æ–≤
        if len(self.timestamps) > self.window_size:
            self.timestamps.pop(0)
    
    def get_fps(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π FPS."""
        if len(self.timestamps) < 2:
            return 0.0
        
        time_diff = self.timestamps[-1] - self.timestamps[0]
        return (len(self.timestamps) - 1) / time_diff if time_diff > 0 else 0.0


def draw_detections(image: np.ndarray, detections: List[dict], 
                    labels: List[str], fps: float) -> np.ndarray:
    """–ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."""
    img_height, img_width = image.shape[:2]
    
    # –ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å FPS
    cv2.putText(
        image,
        f"FPS: {fps:.1f}",
        (10, 30),
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 255, 0),
        2
    )
    
    # –ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏
    for det in detections:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å bbox –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –§–æ—Ä–º–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–∞—à–µ–≥–æ YOLO output - –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å
        bbox = det['bbox']
        
        # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º —Ñ–æ—Ä–º–∞—Ç [ymin, xmin, ymax, xmax] –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π
        y1 = int(bbox[0] * img_height)
        x1 = int(bbox[1] * img_width)
        y2 = int(bbox[2] * img_height)
        x2 = int(bbox[3] * img_width)
        
        # –ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å bbox
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # –ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å label
        class_id = det['class_id']
        score = det['score']
        label = labels[class_id] if class_id < len(labels) else f"class_{class_id}"
        text = f"{label}: {score:.2f}"
        
        cv2.putText(
            image,
            text,
            (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (0, 255, 0),
            2
        )
    
    return image


def main():
    args = parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å
    if not Path(args.model).exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.model}")
        return
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_config(args.config) if Path(args.config).exists() else {}
    labels = load_labels(config)
    
    print("\n" + "="*60)
    print("üöÄ –ó–ê–ü–£–°–ö –î–ï–¢–ï–ö–¶–ò–ò –î–û–†–û–ñ–ù–´–• –ó–ù–ê–ö–û–í")
    print("="*60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–µ–∫—Ç–æ—Ä
    use_edgetpu = config.get('edge_tpu', {}).get('enabled', True)
    detector = TrafficSignDetector(args.model, use_edgetpu=use_edgetpu)
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–∞–º–µ—Ä—É/–∏—Å—Ç–æ—á–Ω–∏–∫
    if args.source:
        print(f"üìπ –ò—Å—Ç–æ—á–Ω–∏–∫: {args.source}")
        cap = cv2.VideoCapture(args.source)
    else:
        print(f"üì∑ –ö–∞–º–µ—Ä–∞ ID: {args.camera}")
        cap = cv2.VideoCapture(args.camera)
    
    if not cap.isOpened():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É/–≤–∏–¥–µ–æ")
        return
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–º–µ—Ä—ã
    camera_config = config.get('camera', {})
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_config.get('resolution', [1920, 1080])[0])
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_config.get('resolution', [1920, 1080])[1])
    cap.set(cv2.CAP_PROP_FPS, camera_config.get('fps', 30))
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ
    video_writer = None
    if args.save_video:
        output_path = Path('deployment/videos') / f"detection_{int(time.time())}.mp4"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        video_writer = cv2.VideoWriter(
            str(output_path),
            fourcc,
            fps,
            (frame_width, frame_height)
        )
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ: {output_path}")
    
    # FPS counter
    fps_counter = FPSCounter()
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–µ–∫—Ü–∏–∏
    confidence_threshold = config.get('model', {}).get('confidence_threshold', 0.6)
    
    print("\n‚ñ∂Ô∏è  –ù–∞—á–∞–ª–æ –¥–µ—Ç–µ–∫—Ü–∏–∏ (–Ω–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞)...")
    print("-" * 60)
    
    frame_count = 0
    detection_count = 0
    
    try:
        while True:
            # –ó–∞—Ö–≤–∞—Ç–∏—Ç—å –∫–∞–¥—Ä
            ret, frame = cap.read()
            if not ret:
                print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä")
                break
            
            frame_count += 1
            
            # –î–µ—Ç–µ–∫—Ü–∏—è
            start_time = time.time()
            detections = detector.detect(frame, confidence_threshold)
            inference_time = (time.time() - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
            
            detection_count += len(detections)
            
            # –û–±–Ω–æ–≤–∏—Ç—å FPS
            fps_counter.update()
            current_fps = fps_counter.get_fps()
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            if args.display or args.save_video:
                frame_with_detections = draw_detections(
                    frame.copy(),
                    detections,
                    labels,
                    current_fps
                )
                
                # –î–æ–±–∞–≤–∏—Ç—å inference time
                cv2.putText(
                    frame_with_detections,
                    f"Inference: {inference_time:.1f}ms",
                    (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 0),
                    2
                )
                
                if args.display:
                    cv2.imshow('Traffic Sign Detection', frame_with_detections)
                
                if video_writer:
                    video_writer.write(frame_with_detections)
            
            # –í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 30 –∫–∞–¥—Ä–æ–≤
            if frame_count % 30 == 0:
                print(f"–ö–∞–¥—Ä—ã: {frame_count} | FPS: {current_fps:.1f} | "
                      f"Inference: {inference_time:.1f}ms | –î–µ—Ç–µ–∫—Ü–∏–π: {detection_count}")
            
            # –í—ã—Ö–æ–¥ –ø–æ 'q'
            if args.display and cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    finally:
        # –û—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã
        cap.release()
        if video_writer:
            video_writer.release()
        if args.display:
            cv2.destroyAllWindows()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("="*60)
        print(f"–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤: {frame_count}")
        print(f"–°—Ä–µ–¥–Ω–∏–π FPS: {fps_counter.get_fps():.1f}")
        print(f"–í—Å–µ–≥–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {detection_count}")
        print(f"–î–µ—Ç–µ–∫—Ü–∏–π –Ω–∞ –∫–∞–¥—Ä: {detection_count/frame_count if frame_count > 0 else 0:.2f}")
        
        print("\n‚ú® –ó–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == '__main__':
    main()
